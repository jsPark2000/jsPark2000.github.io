---
layout: post
title:  "[CS231n] 1. Image Classification"
author: jspark
date: '2021-01-28 12:41:00 +0900'
category: CS231n
---



## 1. Image Classification(이미지 분류)

- CS231n의 [유튜브 강의 내용](https://youtu.be/OoUX-nOEjG0)과 [강의 노트](http://cs231n.github.io/classification)를 참고하여 작성하였습니다.
- 이 포스트는 CS231n - Lecture2 의 내용을 포함합니다.



###   1-1. motivation
---

-  **이미지 분류는 컴퓨터 비전 분야의 핵심 목표중 하나이다.**
   - 어떤 이미지가 주어 졌을 때, 그것을 정해진 카테고리(강아지, 고양이 등)에 할당하는 것



###   1-2. example

---

- **컴퓨터에서의 이미지**
  - 일반적으로 컴퓨터의 이미지는 3차원 배열(3-dimensional array)의 형태이고,                                            각 요소들은 0(black)~255(white) 사이의 값을 갖는다.
  - 예를 들어, 어떤 사진의 크기가 400 x 200 (pixel) 이라 한다면 그 사진은 총 400 x 200 x 3 = 240,000 개의 숫자들로 구성된 것이다.
  - **Image Classificaion**의 목표는 이런 3차원 배열상의 숫자들을 하나의 label(예를 들어 고양이)에 매핑시키는 것이다.
  - 아래는 실제 컴퓨터가 사진을 어떻게 인식하는지 보여주는 예시이다.

![cat](/assets/images/cs231n/cat.png)



###   1-3. challenges

---

- **이미지 분류에서 발생하는 문제**들을 나열해 보았다.
  - **Viewpoint variation**: 같은 물체일지라도 보는 방향에 따라서 달라지는 문제.
  - **Scale variation**: Visual classes often exhibit variation in there size
    - 무슨말인지 잘 모르겠다...
  - **Deformation**: 많은 객체들은 고정된 형태가 없는 경우가 대다수(앉은 고양이, 서있는 고양이 등)
  - **Occlusion**:  객체들의 일부분만 보이는 경우
  - **illumination conditions**: 주변 조명의 영향으로 픽셀값(색상)이 달라져 보이는 경우
  - **Background clutter**: 객체가 주변환경에 섞여서 인식하기 어려운 경우(보호색 등)
  - **Intra-class variation**: 클래스의 범위가 큰 경우(예를 들어 고양이 클래스에는 여러 종의 고양이가 있음) 
  - 강의 노트에서는 아래와 같이 사진으로 정리하여 보여준다.

 ![challenges](/assets/images/cs231n/challenges.jpeg)



###   1-4. 어떻게 Image Classification Algorithm을 작성할 것인가?

---

- 숫자를 정렬하는 알고리즘과 달리 이미지에서 특정 객체를 인식하는 알고리즘을 작성하는 것은 어렵다.
  - 예를 들어 코드를 통해 모든 클래스를 분류 하도록 작성한다고 해보자
  - 위와 같이 할 경우 코드로 작성하지 않은 클래스가 들어오면 반응하기 어렵다.
  - 또한, 입력으로 들어올 수 있는 모든 클래스에 대한 특징들을 코드로 작성하기는 거의 불가능하다.

- 따라서 **Data-Driven Approach** 방식을 사용하게 되었다.



- **Data-Driven Approach**

  - 강의에서는 아래와 같이 설명한다.

    >1. 라벨링된 이미지 데이터 셋을 모은다.
    >2. 머신 러닝을 통해 분류기(Classifier)를 학습시킨다.
    >3. 새로운 이미지(학습 과정에서 사용되지 않은 것)로 학습된 분류기를 평가한다.

  - 정리하자면, 컴퓨터에게 다양한 클래스의 이미지들을 보여주고 머신 러닝 방법을 통해 컴퓨터가 스스로 각 클래스들의 특징(Visual appearance)을 학습하도록 하는 것이다.
  - 아래는 데이터 셋(train set)의 예시이다.

    ![trainset](/assets/images/cs231n/trainset.jpg)





## 2. Nearest Neighbor Classifier

- **Data-Driven Approach** 방식을 사용한 첫 번째 예시를 살펴보자.
- 강의 노트에서는 시작하기에 앞서 이미지 데이터 셋인 CIFAR-10을 소개한다.
- 앞으로는 **Nearest Neighbor를 줄여서 NN**이라고 쓰겠다.



###   2-1. CIFAR-10

---

- Image Classifier를 학습 시킬때 사용하는 데이터 셋의 한 종류이다.

- **10개의 클래스**
- **50,000개의 학습 이미지(training)**
- **10,000개의 평가 이미지(test)**

  ![cf10](/assets/images/cs231n/cf10.jpg)



###  2-2. NN의 동작 원리

---

- NN은 Training시에 모든 학습 이미지들을 저장한다음, <br>Test 시에 이미지가 들어오면, 들어온 이미지와 Training 시에 저장해둔 이미지를 **비교**하여<br>가장 가까운 학습 이미지(저장해둔 이미지)와 동일한 클래스로 분류한다.
- 예를 들어, Training 시에 {고양이, 강아지, 자동차} 3가지 클래스의 이미지들이 들어와서 저장된 상태라 하자.  이제 Test 시에 **이미지 X**가 들어왔을때, NN은 Training 시에 저장한 이미지와 비교하여<br> {고양이} 클래스에 속한 이미지와 가장 가깝다고 판단 하였고, **이미지 X**를 {고양이} 라고 분류한다.
- 그렇다면 여기서 이미지를 **"비교"**한다는 것이 무엇을 의미할까?
- 바로 아래에서 이미지를 비교하는 방법인 **L1 distance** 와 **L2 distance** 에 대해 설명한다. 



#### 2-2-1. L1 distance

---

- 이미지가 2개 있다고 하자, 그러면 두 이미지를 각각 I1, I2 (Vector)로 변환한다.
- 그 다음 아래의 공식처럼 계산하면 L1 distance를 구할 수 있다.

  ![zzzzz](/assets/images/cs231n/zzzzz.PNG)

- 위의 공식만 봐서는 헷갈릴수도 있으니 강의 노트에 있는 사진을 첨부한다.

  ![nneg](/assets/images/cs231n/nneg.jpeg)

  - 그림에서 볼 수 있듯이 두 이미지에서 각 픽셀에 해당하는 숫자의 차이(절댓값)를 계산해서 더하면 된다.
- L1 distance가 작을 수록 두 이미지가 비슷하다고 할 수 있다.
- 따라서 NN은 L1 distance가 가장 작은 training image를 찾고 클래스를 분류(labeling)하는 것이다.



#### 2-2-2. L2 distance

---

- L1 distance를 조금만 변형하면 된다. 
- 각 픽셀 값의 차이를 제곱한 다음 더한 것에 루트를 씌워주면 된다. (유클리드 거리와 비슷함)

  ![zzzzzz](/assets/images/cs231n/zzzzzz.PNG)

- L1 distance 와 비교했을때 다른 결과가 나올 것 같지만<br>square root는 monotonic function(단조 함수) 이고, <br>픽셀값을 더하는 순서(order)가 유지되기 때문에<br>실제로는 NN에서 비슷한 결과를 낸다고 한다.
- **CIFAR-10** 으로 학습 시켰을때 **L1 distance**를 사용한 경우 **38.6%**의 정확도<br>**L2 distance**를 사용한 경우 **35.4%**의 정확도를 보여준다고 한다.



#### 2-2-3. L1 vs L2

----

