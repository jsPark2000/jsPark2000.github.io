---
layout: post
title:  "[CS231n] 4. Backpropagation"
author: jspark
date: '2021-01-30 14:15:00 +0900'
category: CS231n
use_math: true
---

## 1. Backpropagation

- CS231n의 [유튜브 강의 내용](https://youtu.be/d14TUNcbn1k)과 [강의 노트](https://cs231n.github.io/optimization-2/)를 참고하여 작성하였습니다.
- 이 포스트는 CS231n - Lecture3 의 내용을 포함합니다.
- 데스크탑이나 노트북의 경우 글꼴 크기를 확대(ctrl + 마우스 스크롤) 하여 보는 것을 추천합니다.
- 이 글에는 **파이썬 코드가 포함**됩니다.



### 1-1. Introduction
---
- 이번 글에서는 재귀적인 chain rule의 적용을 통해 gradient를 계산하는 방법인 **Backpropagation(오차역전파)**에 대해서 배웁니다.
- **Problem statement**
  - 목표: 함수 **f(x)**에 대해, **x에 대한 f(x)의 gradient**를 구하는것
    - 여기서 **x**는 입력값들의 벡터이다.
- **Motivation**
  - **SVM loss**를 생각해보자.
  - 입력 값은 training data(**x<sub>i</sub>**)와 parameter(가중치, bias)로 구성되어 있다.
  - 하지만 우리는 parameter에 대해서만 컨트롤 할 수 있다.
    - training data는 이미 fixed된 값이 들어오기 때문
  - 따라서, **x<sub>i</sub>**에 대한 gradient 계산이 쉽더라도, 실제로는 우리가 컨트롤 할 수 있는 값인 parameter(가중치, bias)에 대한 gradient만 계산해서 오차 역전파를 수행한다고 한다.
    - **x<sub>i</sub>**(training set)에 대한 gradient를 구해봤자 이미 fixed 되었기 때문에 값을 바꿀 수 없기 때문이다.
  - 때때로는 **x<sub>i</sub>**에 대한 gradient도 유용하게 사용될 수 있다는데 글의 후반 부분에서 다룬다.



## 2. Simple expressions and interpretation of the gradient

- 먼저 간단한 수식을 통해 시작해보자
- 여기 곱셈함수 **f(x, y) = xy**가 있다고 하자, 그러면 x, y 각각에 대한 편미분은 아래와 같다.

$$
f(x,y) = x y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = y \hspace{0.5in} \frac{\partial f}{\partial y} = x
$$



### 2-1. Interpretation
---
- 미분은 입력 변수 부근의 아주 작은(엡실론 근방) 변화에 대한 함수 값의 변화량이라는 것을 기억하자.

$$
\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}
$$

- 위의 식에서 우변은 분수 기호가 나눈다는 의미이지만, 좌변의 경우는 그렇지 않다.
- 연산자 **d/dx**는 앞에오는 함수 **f**가 x에 대해서 미분된 함수라는 의미를 담고있다.
- 위의 수식을 이해하는 좋은 방법은
  - h가 매우 작은 값일 때, 함수 **f**는 직선으로 근사될 수 있고, 미분 값은 근사된 직선의 기울기로 해석하면 된다.
- 이를 앞의 예시인 **f(x, y) = xy**에 대입해보자<br>x = 4, y = -3 이라고 하면 **f(x, y)** = -12가 될 것이다. 여기서 f(x, y)를 x로 편미분한 값은 -3이 된다.<br>이 말은, 만약 x를 조금 증가 시킨다고 하면 **f(x, y)**값은 증가 시킨 x값의 3배 만큼 감소한다는 의미이다.<br>이를 수식으로 표현하면 아래와 같아진다.

$$
f(x + h) = f(x) + h \frac{df(x)}{dx}
$$

- 이제 x 말고 y를 편미분한 값을 위의 식에 넣어보면
  - **f(y+h) = f(y) + 4h** 가 나올 것이다. (x = 4, y = -3)
  - 이는 y값을 증가 시키면, 증가된 y갑의 4배만큼 함수 값이 증가한다는 의미이다.
- 강의 노트에서는 아래와 같이 표현했다.

  > The derivative on each variable tells you the sensitivity of the whole expression on its value.

- 이제 gradient **∇f**를 살펴보자 (**f(x, y) = xy**)
  - 저번 글에서 설명했던 것 처럼, **∇f**는 편미분 값들의 벡터이다.

$$
\nabla f = [\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}] = [y, x]
$$

- gradient **∇f**의 다른 예시들도 살펴보자

$$
f(x,y) = x + y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = 1 \hspace{0.5in} \frac{\partial f}{\partial y} = 1
$$

- 앞에서 사용했던 max 함수에 대한 gradient **∇f**도 구할 수 있다.

$$
f(x,y) = \max(x, y) \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = \mathbb{1}(x >= y) \hspace{0.5in} \frac{\partial f}{\partial y} = \mathbb{1}(y >= x)
$$

- 이 max 함수의 gradient **∇f**를 더 자세히 살펴보자
  - 간단하게는 x, y 둘중에 더 큰 변수만 gradient 값으로 1을 가지고, 작은 변수는 0을 가진다.
  - 이 말은, 만약 x = 4, y = 2 라고 한다면, y의 값을 아주 조금만 증가시켜도 gradient가 0이기 때문에 f(x, y)의 값은 그대로 4일 것이다.
    - y값에 작은 변화에 대해 함수 f(x, y)가 영향을 받지 않는다는 의미이다.
- 하지만, y의 값을 2보다 크게 증가시킨다고 하면 f(x, y) 값에도 변화가 생긴다.<br>그런데, 일반적으로 미분은 매우 작은 변화에 대해서만 생각하기 때문에 이러한 큰 변화(2이상 증가)는 해석하는 의미가 없다.
  
  


## 3. Compound expressions with chain rule
- 이제 더 복잡한 표현들에 대해 알아보자
- 여기서는 **f(x, y, z) = (x + y) * z**를 예시로 사용하겠다.
- 이제 이 함수 **f**를 더 작은 표현으로 쪼개겠다.
  - **q = x + y, f = qz**
- 이렇게 표현한 다음 각각을 편미분 하면 아래와 같다.

$$
\frac{\partial f}{\partial q} = z, \frac{\partial f}{\partial z} = q,
\frac{\partial q}{\partial x} = 1, \frac{\partial q}{\partial y} = 1
$$

- 여기서 q에 대한 편미분 값은 무시해도 된다. 사실상 우리가 관심 있는것은 x, y, z에 대한 **f**의 gradient이기 때문이다.
- 여기서 **연쇄 법칙(Chain rule)**은 gradient 표현식들을 연결하는 적절한 방법이 곱셈이라는 것을 보여준다.

$$
\frac{\partial f}{\partial x} = \frac{\partial f}{\partial q} \frac{\partial q}{\partial x}
$$

- 위와 같은 방식으로 **f(x, y, z) = (x + y) * z**의 x, y, z에 관한 gradient를 모두 구할 수 있다.
- 강의 노트에서는 파이썬 코드로 위의 내용을 설명한다.

```python
# set some inputs
x = -2; y = 5; z = -4

# perform the forward pass
q = x + y # q becomes 3
f = q * z # f becomes -12

# perform the backward pass (backpropagation) in reverse order:
# first backprop through f = q * z
dfdz = q # df/dz = q, so gradient on z becomes 3
dfdq = z # df/dq = z, so gradient on q becomes -4
# now backprop through q = x + y
dfdx = 1.0 * dfdq # dq/dx = 1. And the multiplication here is the chain rule!
dfdy = 1.0 * dfdq # dq/dy = 1
```

- 위의 코드를 보면 x, y, z의 f에 대한 gradient인 **[dfdx, dfdy, dfdz]**를 구한 것을 확인할 수 있다.
  - 이 3개의 gradient는 f에 대한 x, y, z의 민감도를 보여준다.
  - 더 나아가서는 df를 빼고 그냥 [dx, dy, dz]라고 쓸 수도 있다.
  
- 위의 과정을 회로도를 통해 간단하게 나타낼 수 있다.

  ![cs231n_123](C:\Users\idpjs\Desktop\cs231n_123.PNG)

  - 여기서 forward pass는 입력(x, y, z) 부터 출력(f) 까지의 값을 계산한다. (초록색)
  - backward pass는 오차역전파를 수행하는데, 출력에서 시작해서 연쇄법칙을 통해 입력에 대한 gradient 값을 계산한다(빨간색).
    - 각 입력에 대한 gradient 값은 회로를 통해 거꾸로 흐르는 것으로 볼 수 있다.
  
- 위의 회로도를 자세히 살펴보자
  - 먼저 덧셈 게이트에 [-2, 5]가 들어오고 덧셈 계산을 통해 3을 내보낸다.
  - 덧셈 게이트의 경우 위에 gradient 예시에서 보았듯이 local gradient 값이 +1이 된다.
  - 다음으로 곱셉 게이트에는 [3, -4]가 들어오고 계산을 통해 최종 출력 값인 -12를 내보낸다.
  - 이제 backward pass 과정이다
  - 아까 곱셈 함수에 대한 gradient를 보면 덧셈 게이트는 출력 값에 대한 gradient가 -4 라는 것을 학습하게 된다.
    - dfdq = z = -4 이기 때문이다.
    - 강의 노트에서는 전체 회로가 출력값을 최대화 하기를 원한다고 가정하였다.
    - 그렇다면 이 -4라는 값은 덧셈 게이트가 4만큼의 힘(force)으로 게이트의 출력값이 낮아지는 것을 원한다고 볼 수 있다.
  - 반복을 지속하고 gradient 값을 연결하기 위해 덧셈 게이트는 이 gradient 값을 받아들이고 모든 입력들에 대한 local gradient 값에 곱한다.
    - 이 결과 x, y에 대한 gradient 값이 (1 * -4) -4가 된다.
  - 여기서 x,y 가 gradient 값인 -4에 반응하여 감소한다면,<br>덧셈 게이트의 출력은 감소(작아진 값을 더했으므로)할 것이고<br>이는 다시 곱셈 게이트의 출력을 증가하게 만든다.
    - 

