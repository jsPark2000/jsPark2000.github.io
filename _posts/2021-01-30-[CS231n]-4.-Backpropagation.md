---
layout: post
title:  "[CS231n] 4. Backpropagation"
author: jspark
date: '2021-01-30 14:15:00 +0900'
category: CS231n
use_math: true
---

## 1. Backpropagation

- CS231n의 [유튜브 강의 내용](https://youtu.be/d14TUNcbn1k)과 [강의 노트](https://cs231n.github.io/optimization-2/)를 참고하여 작성하였습니다.
- 이 포스트는 CS231n - Lecture3 의 내용을 포함합니다.
- 데스크탑이나 노트북의 경우 글꼴 크기를 확대(ctrl + 마우스 스크롤) 하여 보는 것을 추천합니다.
- 이 글에는 **파이썬 코드가 포함**됩니다.



### 1-1. Introduction
---
- 이번 글에서는 재귀적인 chain rule의 적용을 통해 gradient를 계산하는 방법인 **Backpropagation(오차역전파)**에 대해서 배웁니다.
- **Problem statement**
  - 목표: 함수 **f(x)**에 대해, **x에 대한 f(x)의 gradient**를 구하는것
    - 여기서 **x**는 입력값들의 벡터이다.
- **Motivation**
  - **SVM loss**를 생각해보자.
  - 입력 값은 training data(**x<sub>i</sub>**)와 parameter(가중치, bias)로 구성되어 있다.
  - 하지만 우리는 parameter에 대해서만 컨트롤 할 수 있다.
    - training data는 이미 fixed된 값이 들어오기 때문
  - 따라서, **x<sub>i</sub>**에 대한 gradient 계산이 쉽더라도, 실제로는 우리가 컨트롤 할 수 있는 값인 parameter(가중치, bias)에 대한 gradient만 계산해서 오차 역전파를 수행한다고 한다.
    - **x<sub>i</sub>**(training set)에 대한 gradient를 구해봤자 이미 fixed 되었기 때문에 값을 바꿀 수 없기 때문이다.
  - 때때로는 **x<sub>i</sub>**에 대한 gradient도 유용하게 사용될 수 있다는데 글의 후반 부분에서 다룬다.



## 2. Simple expressions and interpretation of the gradient

- 먼저 간단한 수식을 통해 시작해보자
- 여기 곱셈함수 **f(x, y) = xy**가 있다고 하자, 그러면 x, y 각각에 대한 편미분은 아래와 같다.

$$
f(x,y) = x y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = y \hspace{0.5in} \frac{\partial f}{\partial y} = x
$$



### 2-1. Interpretation
---
- 미분은 입력 변수 부근의 아주 작은(엡실론 근방) 변화에 대한 함수 값의 변화량이라는 것을 기억하자.

$$
\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}
$$

- 위의 식에서 우변은 분수 기호가 나눈다는 의미이지만, 좌변의 경우는 그렇지 않다.
- 연산자 **d/dx**는 앞에오는 함수 **f**가 x에 대해서 미분된 함수라는 의미를 담고있다.
- 위의 수식을 이해하는 좋은 방법은
  - h가 매우 작은 값일 때, 함수 **f**는 직선으로 근사될 수 있고, 미분 값은 근사된 직선의 기울기로 해석하면 된다.
- 이를 앞의 예시인 **f(x, y) = xy**에 대입해보자<br>x = 4, y = -3 이라고 하면 **f(x, y)** = -12가 될 것이다. 여기서 f(x, y)를 x로 편미분한 값은 -3이 된다.<br>이 말은, 만약 x를 조금 증가 시킨다고 하면 **f(x, y)**값은 증가 시킨 x값의 3배 만큼 감소한다는 의미이다.<br>이를 수식으로 표현하면 아래와 같아진다.

$$
f(x + h) = f(x) + h \frac{df(x)}{dx}
$$

- 이제 x 말고 y를 편미분한 값을 위의 식에 넣어보면
  - **f(y+h) = f(y) + 4h** 가 나올 것이다. (x = 4, y = -3)
  - 이는 y값을 증가 시키면, 증가된 y갑의 4배만큼 함수 값이 증가한다는 의미이다.
- 강의 노트에서는 아래와 같이 표현했다.

  > The derivative on each variable tells you the sensitivity of the whole expression on its value.

- 이제 gradient **∇f**를 살펴보자 (**f(x, y) = xy**)
  
  - 저번 글에서 설명했던 것 처럼, **∇f**는 편미분 값들의 벡터이다.

$$
\nabla f = [\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}] = [y, x]
$$

- gradient **∇f**의 다른 예시들도 살펴보자

$$
f(x,y) = x + y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = 1 \hspace{0.5in} \frac{\partial f}{\partial y} = 1
$$

- 앞에서 사용했던 max 함수에 대한 gradient **∇f**도 구할 수 있다.

$$
f(x,y) = \max(x, y) \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = \mathbb{1}(x >= y) \hspace{0.5in} \frac{\partial f}{\partial y} = \mathbb{1}(y >= x)
$$

- 이 max 함수의 gradient **∇f**를 더 자세히 살펴보자
  - 간단하게는 x, y 둘중에 더 큰 변수만 gradient 값으로 1을 가지고, 작은 변수는 0을 가진다.
  - 이 말은, 만약 x = 4, y = 2 라고 한다면, y의 값을 아주 조금만 증가시켜도 gradient가 0이기 때문에 f(x, y)의 값은 그대로 4일 것이다.
    - y값에 작은 변화에 대해 함수 f(x, y)가 영향을 받지 않는다는 의미이다.
- 하지만, y의 값을 2보다 크게 증가시킨다고 하면 f(x, y) 값에도 변화가 생긴다.<br>그런데, 일반적으로 미분은 매우 작은 변화에 대해서만 생각하기 때문에 이러한 큰 변화(2이상 증가)는 해석하는 의미가 없다.
  
  


## 3. Compound expressions with chain rule
- 이제 더 복잡한 표현들에 대해 알아보자
- 여기서는 **f(x, y, z) = (x + y) * z**를 예시로 사용하겠다.
- 이제 이 함수 **f**를 더 작은 표현으로 쪼개겠다.
  - **q = x + y, f = qz**
- 이렇게 표현한 다음 각각을 편미분 하면 아래와 같다.

$$
\frac{\partial f}{\partial q} = z, \frac{\partial f}{\partial z} = q,
\frac{\partial q}{\partial x} = 1, \frac{\partial q}{\partial y} = 1
$$

- 여기서 q에 대한 편미분 값은 무시해도 된다. 사실상 우리가 관심 있는것은 x, y, z에 대한 **f**의 gradient이기 때문이다.
- 여기서 **연쇄 법칙(Chain rule)**은 gradient 표현식들을 연결하는 적절한 방법이 곱셈이라는 것을 보여준다.

$$
\frac{\partial f}{\partial x} = \frac{\partial f}{\partial q} \frac{\partial q}{\partial x}
$$

- 위와 같은 방식으로 **f(x, y, z) = (x + y) * z**의 x, y, z에 관한 gradient를 모두 구할 수 있다.
- 강의 노트에서는 파이썬 코드로 위의 내용을 설명한다.

```python
# set some inputs
x = -2; y = 5; z = -4

# perform the forward pass
q = x + y # q becomes 3
f = q * z # f becomes -12

# perform the backward pass (backpropagation) in reverse order:
# first backprop through f = q * z
dfdz = q # df/dz = q, so gradient on z becomes 3
dfdq = z # df/dq = z, so gradient on q becomes -4
# now backprop through q = x + y
dfdx = 1.0 * dfdq # dq/dx = 1. And the multiplication here is the chain rule!
dfdy = 1.0 * dfdq # dq/dy = 1
```

- 위의 코드를 보면 x, y, z의 f에 대한 gradient인 **[dfdx, dfdy, dfdz]**를 구한 것을 확인할 수 있다.
  - 이 3개의 gradient는 f에 대한 x, y, z의 민감도를 보여준다.
  - 더 나아가서는 df를 빼고 그냥 [dx, dy, dz]라고 쓸 수도 있다.
  
- 위의 과정을 회로도를 통해 간단하게 나타낼 수 있다.

  ![cs231n_123](\assets\images\cs231n\cs231n_123.PNG)

  - df/df = 1 이기 때문에 f 밑에 빨간색 글씨로 1이 써있는것이다.
  - 여기서 forward pass는 입력(x, y, z) 부터 출력(f) 까지의 값을 계산한다. (초록색)
  - backward pass는 오차역전파를 수행하는데, 출력에서 시작해서 연쇄법칙을 통해 출력에 대한 입력의gradient 값을 계산한다(빨간색).
    - 각 입력에 대한 gradient 값은 회로를 통해 거꾸로 흐르는 것으로 볼 수 있다.
  



## 4. Intuitive understanding of backpropagation
- 오차역전파(backpropagation)는 local(지역적) process이다.
- 회로도에서 각 게이트가 input을 받으면 2가지의 값을 즉시 계산할 수 있다.
  - 출력값
  - 출력에 대한 입력의 local gradient
- 위의 2가지의 값을 계산하는 과정은 전체 회로도에 대한 세부사항을 모르는 채로 진행될 수 있다.
- 하지만, forward pass 과정이 끝나면, 게이트는 오차역전파를 통해 전체 회로의 최종 출력에서 출력값의 gradient를 학습하게 된다.
- 연쇄 법칙(chain rule)에 따라 각 게이트에서는 gradient를 가지고 게이트의 모든 입력이 되는 gradient를 구한다.
- 위의 설명을 그림으로 정리해보자

  ![1_xMvDG4P8OM7Qk6n3pW1USA](\assets\images\cs231n\1_xMvDG4P8OM7Qk6n3pW1USA.png)

  - **"전체 회로도에 대한 세부사항을 모르는 채"**
    -  이 부분은 위의 그림에서 볼 수 있듯이 (전체 회로도는 어떻게 생겼는지 모르고) 노드 f는 자신의 주변(local)에 대한 정보만 알고 있다는 것으로 이해할 수 있다.
  - 여기서 **local gradient**는 들어오는 input에 대한 output의 기울기(dz/dx, dz/dy)이다.
  - "**연쇄 법칙(chain rule)에 따라 각 게이트에서는 gradient를 가지고 게이트의 모든 입력이 되는 gradient를 구한다.**"
    - 이 부분은 Downstream gradient를 구하기 위해 Upstream gradient(dL/dz)에<br> local gradient인 (dz/dx)와 (dz/dy)를 곱하는 과정으로 이해할 수 있다.<br>다시 말하면 local input에 대한 출력의 gradient를 구하는 것이다.


- 앞의 회로도를 자세히 살펴보자
  - 먼저 덧셈 게이트에 [-2, 5]가 들어오고 덧셈 계산을 통해 3을 내보낸다.
  - 덧셈 게이트의 경우 위에 gradient 예시에서 보았듯이 local gradient 값이 +1이 된다.
  - 다음으로 곱셉 게이트에는 [3, -4]가 들어오고 계산을 통해 최종 출력 값인 -12를 내보낸다.
  - 이제 backward pass 과정이다
  - 아까 곱셈 함수에 대한 gradient를 보면 덧셈 게이트는 출력 값에 대한 gradient가 -4 라는 것을 학습하게 된다.
    - dfdq = z = -4 이기 때문이다.
    - 강의 노트에서는 전체 회로가 출력값을 최대화 하기를 원한다고 가정하였다.
    - 그렇다면 이 -4라는 값은 덧셈 게이트가 4만큼의 힘(force)으로 게이트의 출력값이 낮아지는 것을 원한다고 볼 수 있다.
  - 반복을 지속하고 gradient 값을 연결하기 위해 덧셈 게이트는 이 gradient 값을 받아들이고 모든 입력들에 대한 local gradient 값에 곱한다.
    - 이 결과 x, y에 대한 gradient 값이 (1 * -4 = ) -4가 된다.
  - 여기서 x,y 가 gradient 값인 -4에 반응하여 감소한다면,<br>덧셈 게이트의 출력은 감소(작아진 값을 더했으므로)할 것이고<br>이는 다시 곱셈 게이트의 출력을 증가하게 만든다.
- 따라서, 오차역전파는 gradient 신호를 통해 출력이 얼마만큼 증가하거나 감소하길 원하는지에 대한 게이트 간의 통신으로 이해할 수 있다.





## 5. Modularity: Sigmoid example
- 앞에서 나온 게이트들은 임의로 설정한 것이라, **미분가능한 함수면 게이트로서 역할을 할 수 있다**고 한다.
- 실제로 미분가능한 함수에 대해, **여러개의 게이트를 그룹지어서 하나의 게이트**로 만들거나,<br>**하나의 함수를 여러개의 게이트로 분해**할 수 있다.
- 아래의 예시를 살펴보자

$$
f(w,x) = \frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}
$$

- 이것은 뒤에서 배우지만 시그모이드 활성화 함수를 이용한 2차원 뉴런의 표현이다.
  - 지금은 그렇게 자세히 알 필요는 없다.
- 단순히 **(W, x)** 입력**x**와 가중치**W**를 입력값으로 받고 하나의 숫자를 output으로 내놓는 함수로 생각하자.
- 이 함수는 아래처럼 여러개의 게이트로 구성되어 있다.

$$
f(x) = \frac{1}{x}
\hspace{1in} \rightarrow \hspace{1in}
\frac{df}{dx} = -1/x^2
\\\\
f_c(x) = c + x
\hspace{1in} \rightarrow \hspace{1in}
\frac{df}{dx} = 1
\\\\
f(x) = e^x
\hspace{1in} \rightarrow \hspace{1in}
\frac{df}{dx} = e^x
\\\\
f_a(x) = ax
\hspace{1in} \rightarrow \hspace{1in}
\frac{df}{dx} = a
$$

- 이 게이트들을 이용한 전체 회로도는 아래와 같다.

  ![dsdfa](\assets\images\cs231n\dsdfa.PNG)

  - 간단하게 입력 값은 입력[x0, x1]과 가중치 [w0, w1, w2]로 구성되어 있다.
  - 먼저 출력 함수 f에 대한 gradient는 df/df = 1 이다.
  - 그 다음 1/x 노드에서 local gradient는 -(1/x)<sup>2</sup>이다.
    - 따라서 이 노드에서 입력에 대한 출력의 gradient는 x에 1.37을 대입하면 -0.53이 된다.
  - 그 다음 + 1 게이트의 경우 local gradient 가 1이 되기 때문에
    - downstream gradient는 1 * -0.53 = -0.53 이된다.
  - exp 게이트의 경우는 local gradient가 e<sup>x</sup>이고, local input 이 -1 이기 때문에
    - downstream gradient는 -0.53 * e<sup>-1</sup> = -0.20 이 된다.
  - *-1 게이트의 경우는 local gradient가 -1 이 된다.
    - downstream gradient는 -0.20 * -1 = 0.20이 된다.
  - 이후 부터는 앞의 예시에서 했던 덧셈 게이트, 곱셈 게이트 2개 밖에 없기 때문에 생략하겠다.


- 여기서 시그모이드 함수의 특징을 알아보자

$$
\sigma(x) = \frac{1}{1+e^{-x}} \\\\
$$

- 위 처럼 생긴 함수를 **시그모이드 함수(sigmoid function)**라고 한다.
- 이 시그모이드 함수의 경우 미분 계산이 아래와 같이 매우 간단하다.

$$
\frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right)
= \left( 1 - \sigma(x) \right) \sigma(x)
$$

- 실제로 그런지 예시를 통해 확인해보자

  ![dddsdf](\assets\images\cs231n\dddsdf.PNG)

  - 아까 보았던 전체 회로도의 뒷부분이다.
  - 이 부분을 sigmoid gate라고 하는데, 실제로 0.73을 가지고 계산해보면,<br>local gradient가 (1 - 0.73)(0.73) ~= 0.2 가 나오는 것을 확인할 수 있다.
  - 앞에서 했었던 복잡한 계산을 한번으로 줄인것이다!

- 이처럼 게이트들을 그룹지어 하나의 게이트로 만드는 것은 때때로 유용하다.



### 5-1. Implementation protip: staged backpropagation
---
- 앞에서 설명한 내용을 코드로 살펴보자

```python
w = [2,-3,-3] # assume some random weights and data
x = [-1, -2]

# forward pass
dot = w[0]*x[0] + w[1]*x[1] + w[2]
f = 1.0 / (1 + math.exp(-dot)) # sigmoid function

# backward pass through the neuron (backpropagation)
ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivation
dx = [w[0] * ddot, w[1] * ddot] # backprop into x
dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w
# we're done! we have the gradients on the inputs to the circuit
```

- 