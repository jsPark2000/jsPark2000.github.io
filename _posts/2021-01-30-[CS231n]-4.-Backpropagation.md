---
layout: post
title:  "[CS231n] 4. Backpropagation"
author: jspark
date: '2021-01-30 14:15:00 +0900'
category: CS231n
use_math: true
---

## 1. Backpropagation

- CS231n의 [유튜브 강의 내용](https://youtu.be/d14TUNcbn1k)과 [강의 노트](https://cs231n.github.io/optimization-2/)를 참고하여 작성하였습니다.
- 이 포스트는 CS231n - Lecture3 의 내용을 포함합니다.
- 데스크탑이나 노트북의 경우 글꼴 크기를 확대(ctrl + 마우스 스크롤) 하여 보는 것을 추천합니다.
- 이 글에는 **파이썬 코드가 포함**됩니다.



### 1-1. Introduction
---
- 이번 글에서는 재귀적인 chain rule의 적용을 통해 gradient를 계산하는 방법인 **Backpropagation(역전파)**에 대해서 배웁니다.
- **Problem statement**
  - 목표: 함수 **f(x)**에 대해, **x에 대한 f(x)의 gradient**를 구하는것
    - 여기서 **x**는 입력값들의 벡터이다.
- **Motivation**
  - **SVM loss**를 생각해보자.
  - 입력 값은 training data(**x<sub>i</sub>**)와 parameter(가중치, bias)로 구성되어 있다.
  - 하지만 우리는 parameter에 대해서만 컨트롤 할 수 있다.
    - training data는 이미 fixed된 값이 들어오기 때문
  - 따라서, **x<sub>i</sub>**에 대한 gradient 계산이 쉽더라도, 실제로는 우리가 컨트롤 할 수 있는 값인 parameter(가중치, bias)에 대한 gradient만 계산해서 오차 역전파를 수행한다고 한다.
    - **x<sub>i</sub>**(training set)에 대한 gradient를 구해봤자 이미 fixed 되었기 때문에 값을 바꿀 수 없기 때문이다.
  - 때때로는 **x<sub>i</sub>**에 대한 gradient도 유용하게 사용될 수 있다는데 글의 후반 부분에서 다룬다.



## 2. Simple expressions and interpretation of the gradient

- 먼저 간단한 수식을 통해 시작해보자
- 여기 곱셈함수 **f(x, y) = xy**가 있다고 하자, 그러면 x, y 각각에 대한 편미분은 아래와 같다.

$$
f(x,y) = x y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = y \hspace{0.5in} \frac{\partial f}{\partial y} = x
$$



### 2-1. Interpretation
---
- 미분은 입력 변수 부근의 아주 작은(엡실론 근방) 변화에 대한 함수 값의 변화량이라는 것을 기억하자.

$$
\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}
$$

- 위의 식에서 우변은 분수 기호가 나눈다는 의미이지만, 좌변의 경우는 그렇지 않다.
- 연산자 **d/dx**는 앞에오는 함수 **f**가 x에 대해서 미분된 함수라는 의미를 담고있다.
- 위의 수식을 이해하는 좋은 방법은
  - h가 매우 작은 값일 때, 함수 **f**는 직선으로 근사될 수 있고, 미분 값은 근사된 직선의 기울기로 해석하면 된다.
- 이를 앞의 예시인 **f(x, y) = xy**에 대입해보자<br>x = 4, y = -3 이라고 하면 **f(x, y)** = -12가 될 것이다. 여기서 f(x, y)를 x로 편미분한 값은 -3이 된다.<br>이 말은, 만약 x를 조금 증가 시킨다고 하면 **f(x, y)**값은 증가 시킨 x값의 3배 만큼 감소한다는 의미이다.<br>이를 수식으로 표현하면 아래와 같아진다.

$$
f(x + h) = f(x) + h \frac{df(x)}{dx}
$$

- 이제 x 말고 y를 편미분한 값을 위의 식에 넣어보면
  - **f(y+h) = f(y) + 4h** 가 나올 것이다. (x = 4, y = -3)
  - 이는 y값을 증가 시키면, 증가된 y갑의 4배만큼 함수 값이 증가한다는 의미이다.
- 강의 노트에서는 아래와 같이 표현했다.

  > The derivative on each variable tells you the sensitivity of the whole expression on its value.

- 이제 gradient **∇f**를 살펴보자 (**f(x, y) = xy**)
  
  - 저번 글에서 설명했던 것 처럼, **∇f**는 편미분 값들의 벡터이다.

$$
\nabla f = [\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}] = [y, x]
$$

- gradient **∇f**의 다른 예시들도 살펴보자

$$
f(x,y) = x + y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = 1 \hspace{0.5in} \frac{\partial f}{\partial y} = 1
$$

- 앞에서 사용했던 max 함수에 대한 gradient **∇f**도 구할 수 있다.

$$
f(x,y) = \max(x, y) \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = \mathbb{1}(x >= y) \hspace{0.5in} \frac{\partial f}{\partial y} = \mathbb{1}(y >= x)
$$

- 이 max 함수의 gradient **∇f**를 더 자세히 살펴보자
  - 간단하게는 x, y 둘중에 더 큰 변수만 gradient 값으로 1을 가지고, 작은 변수는 0을 가진다.
  - 이 말은, 만약 x = 4, y = 2 라고 한다면, y의 값을 아주 조금만 증가시켜도 gradient가 0이기 때문에 f(x, y)의 값은 그대로 4일 것이다.
    - y값에 작은 변화에 대해 함수 f(x, y)가 영향을 받지 않는다는 의미이다.
- 하지만, y의 값을 2보다 크게 증가시킨다고 하면 f(x, y) 값에도 변화가 생긴다.<br>그런데, 일반적으로 미분은 매우 작은 변화에 대해서만 생각하기 때문에 이러한 큰 변화(2이상 증가)는 해석하는 의미가 없다.
  
  


## 3. Compound expressions with chain rule
- 이제 더 복잡한 표현들에 대해 알아보자
- 여기서는 **f(x, y, z) = (x + y) * z**를 예시로 사용하겠다.
- 이제 이 함수 **f**를 더 작은 표현으로 쪼개겠다.
  - **q = x + y, f = qz**
- 이렇게 표현한 다음 각각을 편미분 하면 아래와 같다.

$$
\frac{\partial f}{\partial q} = z, \frac{\partial f}{\partial z} = q,
\frac{\partial q}{\partial x} = 1, \frac{\partial q}{\partial y} = 1
$$

- 여기서 q에 대한 편미분 값은 무시해도 된다. 사실상 우리가 관심 있는것은 x, y, z에 대한 **f**의 gradient이기 때문이다.
- 여기서 **연쇄 법칙(Chain rule)**은 gradient 표현식들을 연결하는 적절한 방법이 곱셈이라는 것을 보여준다.

$$
\frac{\partial f}{\partial x} = \frac{\partial f}{\partial q} \frac{\partial q}{\partial x}
$$

- 위와 같은 방식으로 **f(x, y, z) = (x + y) * z**의 x, y, z에 관한 gradient를 모두 구할 수 있다.
- 강의 노트에서는 파이썬 코드로 위의 내용을 설명한다.

```python
# set some inputs
x = -2; y = 5; z = -4

# perform the forward pass
q = x + y # q becomes 3
f = q * z # f becomes -12

# perform the backward pass (backpropagation) in reverse order:
# first backprop through f = q * z
dfdz = q # df/dz = q, so gradient on z becomes 3
dfdq = z # df/dq = z, so gradient on q becomes -4
# now backprop through q = x + y
dfdx = 1.0 * dfdq # dq/dx = 1. And the multiplication here is the chain rule!
dfdy = 1.0 * dfdq # dq/dy = 1
```

- 위의 코드를 보면 x, y, z의 f에 대한 gradient인 **[dfdx, dfdy, dfdz]**를 구한 것을 확인할 수 있다.
  - 이 3개의 gradient는 f에 대한 x, y, z의 민감도를 보여준다.
  - 더 나아가서는 df를 빼고 그냥 [dx, dy, dz]라고 쓸 수도 있다.
  
- 위의 과정을 회로도를 통해 간단하게 나타낼 수 있다.

  ![cs231n_123](\assets\images\cs231n\cs231n_123.PNG)

  - df/df = 1 이기 때문에 f 밑에 빨간색 글씨로 1이 써있는것이다.
  - 여기서 forward pass는 입력(x, y, z) 부터 출력(f) 까지의 값을 계산한다. (초록색)
  - backward pass는 역전파를 수행하는데, 출력에서 시작해서 연쇄법칙을 통해 출력에 대한 입력의gradient 값을 계산한다(빨간색).
    - 각 입력에 대한 gradient 값은 회로를 통해 거꾸로 흐르는 것으로 볼 수 있다.
  



## 4. Intuitive understanding of backpropagation
- 역전파(backpropagation)는 local(지역적) process이다.
- 회로도에서 각 게이트가 input을 받으면 2가지의 값을 즉시 계산할 수 있다.
  - 출력값
  - 출력에 대한 입력의 local gradient
- 위의 2가지의 값을 계산하는 과정은 전체 회로도에 대한 세부사항을 모르는 채로 진행될 수 있다.
- 하지만, forward pass 과정이 끝나면, 게이트는 역전파를 통해 전체 회로의 최종 출력에서 출력값의 gradient를 학습하게 된다.
- 연쇄 법칙(chain rule)에 따라 각 게이트에서는 gradient를 가지고 게이트의 모든 입력이 되는 gradient를 구한다.
- 위의 설명을 그림으로 정리해보자

  ![1_xMvDG4P8OM7Qk6n3pW1USA](\assets\images\cs231n\1_xMvDG4P8OM7Qk6n3pW1USA.png)

  - **"전체 회로도에 대한 세부사항을 모르는 채"**
    -  이 부분은 위의 그림에서 볼 수 있듯이 (전체 회로도는 어떻게 생겼는지 모르고) 노드 f는 자신의 주변(local)에 대한 정보만 알고 있다는 것으로 이해할 수 있다.
  - 여기서 **local gradient**는 들어오는 input에 대한 output의 기울기(dz/dx, dz/dy)이다.
  - "**연쇄 법칙(chain rule)에 따라 각 게이트에서는 gradient를 가지고 게이트의 모든 입력이 되는 gradient를 구한다.**"
    - 이 부분은 Downstream gradient를 구하기 위해 Upstream gradient(dL/dz)에<br> local gradient인 (dz/dx)와 (dz/dy)를 곱하는 과정으로 이해할 수 있다.<br>다시 말하면 local input에 대한 출력의 gradient를 구하는 것이다.


- 앞의 회로도를 자세히 살펴보자
  - 먼저 덧셈 게이트에 [-2, 5]가 들어오고 덧셈 계산을 통해 3을 내보낸다.
  - 덧셈 게이트의 경우 위에 gradient 예시에서 보았듯이 local gradient 값이 +1이 된다.
  - 다음으로 곱셉 게이트에는 [3, -4]가 들어오고 계산을 통해 최종 출력 값인 -12를 내보낸다.
  - 이제 backward pass 과정이다
  - 아까 곱셈 함수에 대한 gradient를 보면 덧셈 게이트는 출력 값에 대한 gradient가 -4 라는 것을 학습하게 된다.
    - dfdq = z = -4 이기 때문이다.
    - 강의 노트에서는 전체 회로가 출력값을 최대화 하기를 원한다고 가정하였다.
    - 그렇다면 이 -4라는 값은 덧셈 게이트가 4만큼의 힘(force)으로 게이트의 출력값이 낮아지는 것을 원한다고 볼 수 있다.
  - 반복을 지속하고 gradient 값을 연결하기 위해 덧셈 게이트는 이 gradient 값을 받아들이고 모든 입력들에 대한 local gradient 값에 곱한다.
    - 이 결과 x, y에 대한 gradient 값이 (1 * -4 = ) -4가 된다.
  - 여기서 x,y 가 gradient 값인 -4에 반응하여 감소한다면,<br>덧셈 게이트의 출력은 감소(작아진 값을 더했으므로)할 것이고<br>이는 다시 곱셈 게이트의 출력을 증가하게 만든다.
- 따라서, 역전파는 gradient 신호를 통해 출력이 얼마만큼 증가하거나 감소하길 원하는지에 대한 게이트 간의 통신으로 이해할 수 있다.





## 5. Modularity: Sigmoid example
- 앞에서 나온 게이트들은 임의로 설정한 것이라, **미분가능한 함수면 게이트로서 역할을 할 수 있다**고 한다.
- 실제로 미분가능한 함수에 대해, **여러개의 게이트를 그룹지어서 하나의 게이트**로 만들거나,<br>**하나의 함수를 여러개의 게이트로 분해**할 수 있다.
- 아래의 예시를 살펴보자

$$
f(w,x) = \frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}
$$

- 이것은 뒤에서 배우지만 시그모이드 활성화 함수를 이용한 2차원 뉴런의 표현이다.
  - 지금은 그렇게 자세히 알 필요는 없다.
- 단순히 **(W, x)** 입력**x**와 가중치**W**를 입력값으로 받고 하나의 숫자를 output으로 내놓는 함수로 생각하자.
- 이 함수는 아래처럼 여러개의 게이트로 구성되어 있다.

$$
f(x) = \frac{1}{x}
\hspace{1in} \rightarrow \hspace{1in}
\frac{df}{dx} = -1/x^2
\\\\
f_c(x) = c + x
\hspace{1in} \rightarrow \hspace{1in}
\frac{df}{dx} = 1
\\\\
f(x) = e^x
\hspace{1in} \rightarrow \hspace{1in}
\frac{df}{dx} = e^x
\\\\
f_a(x) = ax
\hspace{1in} \rightarrow \hspace{1in}
\frac{df}{dx} = a
$$

- 이 게이트들을 이용한 전체 회로도는 아래와 같다.

  ![dsdfa](\assets\images\cs231n\dsdfa.PNG)

  - 간단하게 입력 값은 입력[x0, x1]과 가중치 [w0, w1, w2]로 구성되어 있다.
  - 먼저 출력 함수 f에 대한 gradient는 df/df = 1 이다.
  - 그 다음 1/x 노드에서 local gradient는 -(1/x)<sup>2</sup>이다.
    - 따라서 이 노드에서 입력에 대한 출력의 gradient는 x에 1.37을 대입하면 -0.53이 된다.
  - 그 다음 + 1 게이트의 경우 local gradient 가 1이 되기 때문에
    - downstream gradient는 1 * -0.53 = -0.53 이된다.
  - exp 게이트의 경우는 local gradient가 e<sup>x</sup>이고, local input 이 -1 이기 때문에
    - downstream gradient는 -0.53 * e<sup>-1</sup> = -0.20 이 된다.
  - *-1 게이트의 경우는 local gradient가 -1 이 된다.
    - downstream gradient는 -0.20 * -1 = 0.20이 된다.
  - 이후 부터는 앞의 예시에서 했던 덧셈 게이트, 곱셈 게이트 2개 밖에 없기 때문에 생략하겠다.


- 여기서 시그모이드 함수의 특징을 알아보자

$$
\sigma(x) = \frac{1}{1+e^{-x}} \\\\
$$

- 위 처럼 생긴 함수를 **시그모이드 함수(sigmoid function)**라고 한다.
- 이 시그모이드 함수의 경우 미분 계산이 아래와 같이 매우 간단하다.

$$
\frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right)
= \left( 1 - \sigma(x) \right) \sigma(x)
$$

- 실제로 그런지 예시를 통해 확인해보자

  ![dddsdf](\assets\images\cs231n\dddsdf.PNG)

  - 아까 보았던 전체 회로도의 뒷부분이다.
  - 이 부분을 sigmoid gate라고 하는데, 실제로 0.73을 가지고 계산해보면,<br>local gradient가 (1 - 0.73)(0.73) ~= 0.2 가 나오는 것을 확인할 수 있다.
  - 앞에서 했었던 복잡한 계산을 한번으로 줄인것이다!

- 이처럼 게이트들을 그룹지어 하나의 게이트로 만드는 것은 때때로 유용하다.



### 5-1. Implementation protip: staged backpropagation
---
- 앞에서 설명한 내용을 코드로 살펴보자

```python
w = [2,-3,-3] # assume some random weights and data
x = [-1, -2]

# forward pass
dot = w[0]*x[0] + w[1]*x[1] + w[2]
f = 1.0 / (1 + math.exp(-dot)) # sigmoid function

# backward pass through the neuron (backpropagation)
ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivation
dx = [w[0] * ddot, w[1] * ddot] # backprop into x
dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w
# we're done! we have the gradients on the inputs to the circuit
```

- 위의 코드에서 forward pass를 쉽게 오차 역전파되는 단계들로 분해하였다.
  - w 와 x의 내적(dot product) 값을 담는 dot 변수를 선언한데서 확인할 수 있다.
  - 이를 통해 backward pass 과정에서 쉽게 dx, dw를 구할 수 있다.
    - dx, dw는 입력값  w, x에 대한 f의 gradient인 df/dx, df/dw 에서 df를 생략한것

- 5장에서는 어떻게 역전파가 수행되는지, forward function의 어느 부분을 게이트로 설정해야 되는지는 편의사항이라는 것이다.
  - 어떻게 정하는지에 따라 적은 코드와 계산 과정으로 역전파 과정을 수행할 수 있게된다. 





## 6. Backprop in practice: Staged computation

- 다른 예제를 살펴보자

$$
f(x,y) = \frac{x + \sigma(y)}{\sigma(x) + (x+y)^2}
$$

- 이런 함수가 있다고 치면 x, y 값에 대한 f(x, y)의 gradient를 구하는 것은 매우 복잡한 과정을 필요로 할 것이다.
- 실제로 gradient를 구한다고 저런 함수를 미분할 필요는 없다.
- 단지, 어떻게 계산하는지만 알면 되기 때문이다.
- 코드를 통해 저런 수식들에 대해 forward pass를 구조화 하는지 알아보자

```python
x = 3 # example values
y = -4

# forward pass
sigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator   #(1)
num = x + sigy # numerator                               #(2)
sigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)
xpy = x + y                                              #(4)
xpysqr = xpy**2                                          #(5)
den = sigx + xpysqr # denominator                        #(6)
invden = 1.0 / den                                       #(7)
f = num * invden # done!                                 #(8)
```

- 위의 코드에서 마지막에 forward pass를 계산하였다.
- sigy, sigx 처럼 우리가 이미 local gradient 값을 알고 있는 다수의 중간 변수를 포함하는 방식으로 위의 복잡한 수식을 나타낸 사실에 주목하자.
- 저렇게 표현하면 직접 x, y에 대해 gradient를 구할 때 보다 역전파 과정의 계산이 쉬워진다.
- 이제 역전파 과정에 대한 코드를 보자

```python
# backprop f = num * invden
dnum = invden # gradient on numerator                             #(8)
dinvden = num                                                     #(8)
# backprop invden = 1.0 / den
dden = (-1.0 / (den**2)) * dinvden                                #(7)
# backprop den = sigx + xpysqr
dsigx = (1) * dden                                                #(6)
dxpysqr = (1) * dden                                              #(6)
# backprop xpysqr = xpy**2
dxpy = (2 * xpy) * dxpysqr                                        #(5)
# backprop xpy = x + y
dx = (1) * dxpy                                                   #(4)
dy = (1) * dxpy                                                   #(4)
# backprop sigx = 1.0 / (1 + math.exp(-x))
dx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below  #(3)
# backprop num = x + sigy
dx += (1) * dnum                                                  #(2)
dsigy = (1) * dnum                                                #(2)
# backprop sigy = 1.0 / (1 + math.exp(-y))
dy += ((1 - sigy) * sigy) * dsigy                                 #(1)
# done! phew
```

- 위 처럼 dx, dy에 대한 f(x, y)의 gradient를 직접 미분하지 않고 재귀적인 연쇄법칙의 적용을 통해 이미 알고 있는 local gradient들의 곱으로 더 쉬운 방법으로 구할 수 있다.
- 강의 노트에서는 몇가지 **주의사항**에 대해 언급하였다.
- **Cache forward pass variables**
  - backward pass 계산 과정에서 forward pass에 사용됬던 변수들을 저장하고(cache) 있는 것은 도움이 된다고 한다.
- **Gradients add up at forks**
  - forward expression들은 x,y를 여러번 포함하기 때문에,<br>역전파를 수행할때 gradient를 축적하기 위해 **+=**를 사용하는 경우가 있는데, 이때 **+=**를 **=**로 쓰지 않도록 주의해야 한다.
  - 미적분학에서는 하나의 변수가 회로의 다른 부분들로 가지를 뻗어나가면, 반환되는 gradient의 값들은 더해질 것이라고 명시되어져 있다고 한다.



## 7. Patterns in backward flow

- 역전파를 수행할 때 자주 보게되는 패턴들에 대해 알아보자

  ![dssasdf](\assets\images\cs231n\dssasdf.PNG)

  - 위의 사진은 CS231n - Lecture 4의 강의 슬라이드를 가져온 것이다.
  - 위의 회로도에는 가장 흔하게 볼 수 있는 게이트인
    - **덧셈, 곱셈, max 게이트** 3개가 나와있다.
  - 이제 이 3개의 게이트의 특징을 알아보자
  - **덧셈(add) 게이트**
    - 일단 덧셈 게이트의 local gradient는 +1 이다.
    - 이것 때문에 출력에 대한 gradient에 1이 곱해져 그대로 입력의 gradient가 된다.
      - 위에서는 둘다 2.0으로 같다는 것을 알 수 있다.
  - **최대값(max) 게이트**
    - 들어온 입력중 가장 큰 값을 출력으로 내보내는 게이트이다.
    - 덧셈 게이트는 출력의 gradient를 그대로 입력의 gradient로 보냈던 반면
    - 최대값 게이트의 local gradient는 가장 큰 값에만 그대로 gradient를 보낸다.
    - 나머지 값들에 대해서는 0을 내보낸다.
      - 위에서는 2와 -1이 max 게이트로 들어왔는데, z 변수에만 2.0 그대로 gradient를 내보냈고,<br>w변수에는 0을 내보낸 것을 알 수 있다.
  - **곱셈(mul) 게이트**
    - 곱셈 게이트의 경우 조금 특이하다.
    - local gradient 값이 입력 값이다. (스위치 된것은 제외)
    - 위에서 x의 gradient는 2 (upstream gradient) 에 -4가 곱해진 -8.0 이다. 



### 7-1. Unintuitive effects and their consequences
---
- 바로 앞에서 곱셈 게이트는 조금 특이하다고 했었다.
- 곱셈 게이트가 있고 2개의 입력을 받을 때,<br>한 쪽의 입력은 매우 크고, 다른 쪽의 입력은 매우 작다고 해보자
- 그러면 곱셈 게이트는 약간 직관적이지 않은 행동을 한다
  - 작은 쪽의 입력에 큰 gradient를 주고
  - 큰 쪽의 입력에 작은 gradient를 준다.
- 전에 linear classifier 내용중에서
  - **w<sup>T</sup>x<sub>i</sub>** 이런식으로 나타났던 것을 기억해보자
  - 위의 내용과 연관지어 보면, 여기서 곱셈 게이트가 직관적이지 않은 행동을 한다는 것은<br>**x<sub>i</sub>**의 크기가 가중치 **w**의 gradient에 영향을 미친다는 것을 의미한다.
  - 이는, 전처리 과정에서 **x<sub>i</sub>**에 1000을 곱하면 가중치 **w**의 gradient는 1000배가 되고,<br>그만큼 학습률을 낮춰야 한다는 뜻이다.
    - 이는 전처리 과정이 중요하다는 의미로도 해석할 수 있다.





## 8. Gradients for vectorized operations
- 앞에 까지의 내용은 변수 하나에 관한 것이었다.
- 지금부터는 앞의 내용을 **행렬과 벡터 연산으로 확장**시킨다.

```python
# forward pass(순전파)
W = np.random.randn(5, 10)
X = np.random.randn(10, 3)
D = W.dot(X)

# now suppose we had the gradient on D from above in the circuit
dD = np.random.randn(*D.shape) # same shape as D
dW = dD.dot(X.T) # .T gives the transpose of the matrix
dX = W.T.dot(dD)
```

- Tip: 차원 분석을 활용해라!
  - dW, dX의 표현은 차원을 이용하여 다시 유도하기 쉽기 때문에, 이 둘의 표현을 외울 필요는 없다.
  - 예를 들어, 위에서 X와 dD의 행렬 곱으로 구해는 **가중치의 gradient W**는 **W**의 크기와 같아야 한다.
  - 행렬의 차원을 일치하게 만드는 방법은 한가지 뿐이다.
  - 예를 들어, 위에서 **X**는 [10 x 3] 행렬이고, **dD**는 [5 x 3]의 크기를 가지면,<br>**dW**와 **W**가 [5 x 10]의 크기를 가지기 위해서는 dD.dot(X.T) 를 사용해야만 가능하다.
- **Work with samll, explicit examples**
  - 처음에는 벡터화된 표현식에서 gradient 업데이트를 유도하는 것은 어려울 수 있다.
  - 그래서, 작고 간단한 예시를 종이에 써가면서 패턴을 익히고, 그 패턴을 일반화 시켜서 벡터화된 표현식에 적용하는 방법을 추천한다.
  - 자세한 내용은 [이곳](http://cs231n.stanford.edu/vecDerivs.pdf)에 나와있다.





## 9. 글을 마치며
- gradient의 의미와, 역전파의 단계별 연산을 살펴보면서, 실제 복잡한 미분 공식없이 우리가 이미 local gradient 값을 알고 있는 게이트들로 전체 식을 나누어서 계산하는 방법을 살펴보았다.
- 다음 글에서는 합성곱 신경망(ConvNets)에 대해 쓸 예정이다. 