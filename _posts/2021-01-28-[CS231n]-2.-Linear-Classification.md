---
layout: post
title:  "[CS231n] 2. Linear Classification"
author: jspark
date: '2021-01-28 16:15:00 +0900'
category: CS231n
use_math: true
---


## 1. Linear Classification(선형 분류)

- CS231n의 [유튜브 강의 내용](https://youtu.be/OoUX-nOEjG0)과 [강의 노트](https://cs231n.github.io/linear-classify/)를 참고하여 작성하였습니다.
- 이 포스트는 CS231n - Lecture2~3 의 내용을 포함합니다.
- 데스크탑이나 노트북의 경우 글꼴 크기를 확대(ctrl + 마우스 스크롤) 하여 보는 것을 추천합니다.
- 파이썬 코드는 글의 내용에 포함되지 않습니다.



### 1-1. Overview

---

- 지난 시간에 배웠던 Image Classificaion을 확장시켜 Neural Network와 Convolution Neural Network를 배우는 Section이다.
- 자세히는<br>**score function**: raw data를 class score로 매핑해주는 function.<br>**loss function**: 모델의 predicted score와 실제 label의 차이를 보여주는 function.<br>**Optimization Problem**: score function의 parameter에 대한 loss function을 최소화 하는것<br>에 대한 내용을 다룬다.



## 2. Parameterized mapping from images to label scores

- **Score function**
  - 이미지들의 픽셀값을 각 클래스들의 점수로 매핑해주는 함수.
  - 이미지 X 가 있고 클래스가 {고양이, 강아지, 배} 3개가 있다고 한다면 score function은<br> 이미지 X를 (4, 6, 2) 와 같이 각 클래스들에 대한 점수로 매핑해준다.
- 앞으로는 강의 교안에서 나오는 예시로 설명하도록 하겠다.

학습 데이터셋 이미지들인

$$
x_i \in R^D
$$

가 있고, 각각이 해당 라벨

$$
y_i
$$

를 갖고 있다고 하자. 여기서

$$
i = 1 \dots N,   y_i \in \{ 1 \dots K \}
$$

이다.

  학습할 데이터 **N** 개가 있고 (각각은 **D** 차원의 벡터이다.), 총 **K** 개의 서로 다른 카테고리(클래스)가 있다.<br>  예를 들어, CIFAR-10 에서는 **N** = 50,000 개의 학습 데이터 이미지들이 있고,<br>  각각은 **D** = 32 x 32 x 3 = 3072 픽셀로 이루어져 있으며, (dog, cat, car, 등등)<br>  10개의 서로 다른 클래스가 있으므로 **K** = 10 이다.    

- **Linear Classifier**
    - 강의 노트에서는 가장 단순한 함수인 linear mapping 부터 시작한다.

$$
f(x_i, W, b) =  W x_i + b
$$

- 위 식에서는 이미지 **X<sub>i</sub>**가 열 벡터 형태 [D x 1]로 나타날 수 있다는 것을 가정하였다.

- 행렬 **W**([K x D])와 벡터 **b**는 위 함수의 parameter라고 할 수 있다. (우리가 정해야 하기 때문)

  - **W**는 가중치(**weights**), **b** 편향(**bias vector**)라고 부르기도 한다.
  - 이 둘은 입력 데이터(x<sub>i</sub>)와 상호작용없이 output score에 영향을 미친다.

- 밑에는 강의 노트에서 언급한 내용을 적어보았다.

  >한 번의 행렬곱 **W**x<sub>i</sub> 만으로 10개의 다른 분류기(각 클래스마다 하나씩)를 병렬로 계산하는 효과를 나타내고 있다. 이 때 **W**의 각 열이 하나의 분류기 역할을 하고 있다고 보면된다.
  >
  >Parameter인 **W, b**를 우리가 조절할 수 있다. 실제로, 우리가 해야할 일은 score function의 output 실제 label과 최대한 일치하도록 이 parameter들을 조절하는 것이다.
  >
  >이러한 방식의 장점은 학습(training)이 끝난 이후에 **W, b**만 남기 때문에 학습시에 사용했던 데이터들이 필요 없어 빠르게 test를 진행 할 수 있다는 것이다.
  >
  >마지막으로, 행렬곱 연산 **W**x<sub>i</sub> 한번과 **b**(bias)를 한번 더해주면 끝나기 때문에<br>입력된 모든 데이터 L1혹은 L2 distance를 계산하는 것과 비교하면 매우 빠르다는 것을 알 수 있다. 



### 2-1. Interpreting a linear classifier

---

