---
layout: post
title:  "[CS231n] 5. Neural Networks Part1"
author: jspark
date: '2021-01-31 14:15:00 +0900'
category: CS231n
use_math: true
---

## 1. Neural Networks Part1

- CS231n의 [유튜브 강의 내용](https://youtu.be/bNb2fEVKeEo)과 [강의 노트](https://cs231n.github.io/neural-networks-1/)를 참고하여 작성하였습니다.
- 이 포스트는 CS231n - Lecture 6,7 의 내용을 포함합니다.
- 데스크탑이나 노트북의 경우 글꼴 크기를 확대(ctrl + 마우스 스크롤) 하여 보는 것을 추천합니다.
- 이 글에는 **파이썬 코드가 포함**됩니다.



### 1-1. Quick Intro
---
- 신경망을 뇌(brain analogy)와 관련된 지식 없이 설명해보자.
- 이전 section에서 **linear classification**에 대한 내용을 다룰 때,<br>image의 픽셀 값들을 서로 다른 클래스 점수로 매핑하던 것을 기억해보자.
- 간단하게 **s = Wx**와 같은 형태로 계산했었다.
  - **W**는 가중치이고, **x**는 열 벡터 형태로 주어지는 입력 image.
  - **CIFAR-10**을 사용할 경우 10개의 클래스에 대한 각각의 점수가 나왔었다.
- 이와 다르게 **Neural Networks(신경망)**의 경우 **s = W<sub>2</sub>max(0, W<sub>1</sub>x)**와 같은 형태의 계산을 수행한다.
  - 위에서 **x**가 [3072 x 1] 형태의 입력이고, **W<sub>1</sub>**이 [100 x 3072] 형태를 가진다고 하자.
  - 그러면 W<sub>1</sub>x는 입력 이미지 x를 100-차원의 벡터로 변환할 것이다.
  - 여기서 **max(0, -)**는 비선형 함수로 작용한다.
  - **W<sub>2</sub>**는 [10 x 100] 이라 해보자.
  - 그래서 결국에는 10-차원 열벡터, 즉 10개의 클래스 점수로 나타난다.
- 여기서 **비선형성(non-linearity)가 중요**한데,
- 이러한 비선형성이 없으면, 두 행렬이 곱해져서 하나의 행렬이 되고,<br>결국에는 **클래스에 대한 예측 점수가 입력값에 대한 선형 함수**가 되어버리기 때문이다.
- 즉, 비선형 함수를 통해 흔들림(wiggle)을 얻을 수 있다.
-  **s = W<sub>2</sub>max(0, W<sub>1</sub>x)**에서 **W<sub>1</sub>, W<sub>2</sub>**는 parameter로서 SGD(확률적 경사하강)로 학습된다.
  - 여기서 gradient는 전에 배운 chain rule을 이용한 역전파를 통해 계산한다.



- 3-layer 신경망의 경우를 살펴보면
- **s = W<sub>3</sub> max(0, W<sub>2</sub> max(0, W<sub>1</sub>x))**와 같은 형태로 나타난다.
- **W<sub>3</sub>, W<sub>2</sub>, W<sub>1</sub>**이 학습해야 할 parameter 값이다.
- 중간에 은닉(hidden) 벡터의 크기도 네트워크의 hyperparameter 라서 설정해 주어야하는 값인데,<br>이후에 어떻게 설정하는지 알아본다.
- 이제 본격적으로 시작해보자!\



## 2. Modeling one neuron
- 신경망 분야는 처음에는 생물학적 신경계를 모방한다는 목표로 시작하였다.
- 하지만, 최근에는 공학적 문제로 확장되었고, 머신 러닝 분야에서 좋은 결과를 내고 있다.
- 그래서 2장은 초기에 많은 영향을 끼쳤던 생물학적 내용에 대해 다루면서 시작한다.



### 2-1. Biological motivation and connections
---
- 주요 관심사가 아니니, 사진 한장으로 간단하게 정리해 보도록 하겠다.

  ![6.2.1.neuron](\assets\images\cs231n\6.2.1.neuron.png)

  - 위의 사진이 신경계를 구성하는 기본 단위인 뉴런이다.
  - 이제 이것을 수학적으로 모델링 한것이 아래와 같다.

  ![6.2.1.neuron_model](\assets\images\cs231n\6.2.1.neuron_model.jpeg)

  - 뉴런에서 다른 뉴런으로 신호는 보내는 것은 위의 모델에서 **활성화 함수(activation function)**에 해당한다.
  - 위의 모델을 파이썬 코드로 나타내 보자.

```python
class Neuron(object):
  # ... 
  def forward(self, inputs):
    """ assume inputs and weights are 1-D numpy arrays and bias is a number """
    cell_body_sum = np.sum(inputs * self.weights) + self.bias
    firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function
    return firing_rate
```

- 위의 코드에서 기억해야 할 것은, 내적을 한다음 bias를 더하고서 **비선형 함수 sigmoid를 적용** 했다는 것이다.
- **σ(x) = 1/(1 + e<sup>-x</sup>)** : 시그모이드 함수
- 이 시그모이드 함수는 활성화 함수(activation function)로 자주 쓰이는 함수이다.
- 이 글의 후반부에서 다른 활성화 함수에 대해서도 다룬다.
- 위의 모델은 생물학적 신경계를 매우 단순하게 표현한 것이기 때문에, 실제 뇌와 비교하는 것은 무리가 있다.




### 2-2. Single neuron as a linear classifier
---
- 이전의 linear classifier에서 보았듯이, 뉴런은 입력 공간의 특정 선형 영역을 "선호한다(활성화 함수값이 1에 가까움)" 혹은 "선호 하지 않는다(활성화 함수값이 0에 가까움)"라고 할 수 있었다.
- 이 말은 뉴런의 output에 손실 함수를 추가하면 하나의 뉴런을 linear classifier로 바꿀 수 있다는 이야기다.



- **Binary Softmax classifier**
  - 이전에 softmax 함수에 대해 배울때를 떠올려보자.

    ![ccxcv](\assets\images\cs231n\ccxcv.PNG)

  - 위와 같은 식을 **P(y<sub>i</sub> = 1 ㅣ x<sub>i</sub> ; w)**
  - 즉, 한 클래스에 대한 확률로 해석 해볼 수 있다.
  - 여기서는 **이항 분류(Binary)** 이기 때문에, 정답과 정답이 아닌 클래스 2개만 존재하여
  - 정답인 클래스의 확률이 **P(y<sub>i</sub> = 1 ㅣ x<sub>i</sub> ; w)**이라면
  - 정답이 아닌 클래스의 확률은 **P(y<sub>i</sub> = 0 ㅣ x<sub>i</sub> ; w) = 1 - P(y<sub>i</sub> = 1 ㅣx<sub>i</sub> ; w)**이다.
  - 둘의 확률을 더하면 1이 되는 것을 확인하자.
  - 여기서는 sigmoid 함수를 적용했기 때문에, 0~1 사이의 값이 결과로 나온다.
  - 따라서, 이것을 사용한 classifier의 예측은 중간 값인 0.5보다 큰지에 근거한다.
    - 간단하게, 0.5 보다 작으면 정답이 아니다, 0.5보다 크면 정답이다 라고 생각해 볼 수 있다.
- **Binary SVM classifier**
  - 뉴런의 출력에 max-margin hinge loss를 적용하여 Binary SVM이 되도록 훈련시킬 수 있다.
- **Regularization interpretation**
  - SVM/Softmax에서 규제 손실은 점진적 망각(gradual forgetting)에 비유할 수 있다고 한다.
  - 매 parameter 업데이트시 마다 시냅스 가중치 **w**가 0을 향해 줄어들기 때문이라고 한다.

- 한 줄로 정리하면 아래와 같다.

  > A single neuron can be used to implement a binary classifier



### 2-3. Commonly used activation functions
---
- 