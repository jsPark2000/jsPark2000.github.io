---
layout: post
title:  "[CS231n] 5. Neural Networks Part1"
author: jspark
date: '2021-01-31 14:15:00 +0900'
category: CS231n
use_math: true
---

## 1. Neural Networks Part1

- CS231n의 [유튜브 강의 내용](https://youtu.be/bNb2fEVKeEo)과 [강의 노트](https://cs231n.github.io/neural-networks-1/)를 참고하여 작성하였습니다.
- 이 포스트는 CS231n - Lecture 6,7 의 내용을 포함합니다.
- 데스크탑이나 노트북의 경우 글꼴 크기를 확대(ctrl + 마우스 스크롤) 하여 보는 것을 추천합니다.
- 이 글에는 **파이썬 코드가 포함**됩니다.



### 1-1. Quick Intro
---
- 신경망을 뇌(brain analogy)와 관련된 지식 없이 설명해보자.
- 이전 section에서 **linear classification**에 대한 내용을 다룰 때,<br>image의 픽셀 값들을 서로 다른 클래스 점수로 매핑하던 것을 기억해보자.
- 간단하게 **s = Wx**와 같은 형태로 계산했었다.
  - **W**는 가중치이고, **x**는 열 벡터 형태로 주어지는 입력 image.
  - **CIFAR-10**을 사용할 경우 10개의 클래스에 대한 각각의 점수가 나왔었다.
- 이와 다르게 **Neural Networks(신경망)**의 경우 **s = W<sub>2</sub>max(0, W<sub>1</sub>x)**와 같은 형태의 계산을 수행한다.
  - 위에서 **x**가 [3072 x 1] 형태의 입력이고, **W<sub>1</sub>**이 [100 x 3072] 형태를 가진다고 하자.
  - 그러면 W<sub>1</sub>x는 입력 이미지 x를 100-차원의 벡터로 변환할 것이다.
  - 여기서 **max(0, -)**는 비선형 함수로 작용한다.
  - **W<sub>2</sub>**는 [10 x 100] 이라 해보자.
  - 그래서 결국에는 10-차원 열벡터, 즉 10개의 클래스 점수로 나타난다.
- 여기서 **비선형성(non-linearity)가 중요**한데,
- 이러한 비선형성이 없으면, 두 행렬이 곱해져서 하나의 행렬이 되고,<br>결국에는 **클래스에 대한 예측 점수가 입력값에 대한 선형 함수**가 되어버리기 때문이다.
- 즉, 비선형 함수를 통해 흔들림(wiggle)을 얻을 수 있다.
-  **s = W<sub>2</sub>max(0, W<sub>1</sub>x)**에서 **W<sub>1</sub>, W<sub>2</sub>**는 parameter로서 SGD(확률적 경사하강)로 학습된다.
  - 여기서 gradient는 전에 배운 chain rule을 이용한 역전파를 통해 계산한다.



- 3-layer 신경망의 경우를 살펴보면
- **s = W<sub>3</sub> max(0, W<sub>2</sub> max(0, W<sub>1</sub>x))**와 같은 형태로 나타난다.
- **W<sub>3</sub>, W<sub>2</sub>, W<sub>1</sub>**이 학습해야 할 parameter 값이다.
- 중간에 은닉(hidden) 벡터의 크기도 네트워크의 hyperparameter 라서 설정해 주어야하는 값인데,<br>이후에 어떻게 설정하는지 알아본다.
- 이제 본격적으로 시작해보자!\



## 2. Modeling one neuron
- 신경망 분야는 처음에는 생물학적 신경계를 모방한다는 목표로 시작하였다.
- 하지만, 최근에는 공학적 문제로 확장되었고, 머신 러닝 분야에서 좋은 결과를 내고 있다.
- 그래서 2장은 초기에 많은 영향을 끼쳤던 생물학적 내용에 대해 다루면서 시작한다.



### 2-1. Biological motivation and connections
---
- 주요 관심사가 아니니, 사진 한장으로 간단하게 정리해 보도록 하겠다.

  ![6.2.1.neuron](\assets\images\cs231n\6.2.1.neuron.png)

  - 위의 사진이 신경계를 구성하는 기본 단위인 뉴런이다.
  - 이제 이것을 수학적으로 모델링 한것이 아래와 같다.

  ![6.2.1.neuron_model](\assets\images\cs231n\6.2.1.neuron_model.jpeg)

  - 뉴런에서 다른 뉴런으로 신호는 보내는 것은 위의 모델에서 **활성화 함수(activation function)**에 해당한다.
  - 위의 모델을 파이썬 코드로 나타내 보자.

```python
class Neuron(object):
  # ... 
  def forward(self, inputs):
    """ assume inputs and weights are 1-D numpy arrays and bias is a number """
    cell_body_sum = np.sum(inputs * self.weights) + self.bias
    firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function
    return firing_rate
```

- 위의 코드에서 기억해야 할 것은, 내적을 한다음 bias를 더하고서 **비선형 함수 sigmoid를 적용** 했다는 것이다.
- **σ(x) = 1/(1 + e<sup>-x</sup>)** : 시그모이드 함수
- 이 시그모이드 함수는 활성화 함수(activation function)로 자주 쓰이는 함수이다.
- 이 글의 후반부에서 다른 활성화 함수에 대해서도 다룬다.
- 위의 모델은 생물학적 신경계를 매우 단순하게 표현한 것이기 때문에, 실제 뇌와 비교하는 것은 무리가 있다.




### 2-2. Single neuron as a linear classifier
---
- 이전의 linear classifier에서 보았듯이, 뉴런은 입력 공간의 특정 선형 영역을 "선호한다(활성화 함수값이 1에 가까움)" 혹은 "선호 하지 않는다(활성화 함수값이 0에 가까움)"라고 할 수 있었다.
- 이 말은 뉴런의 output에 손실 함수를 추가하면 하나의 뉴런을 linear classifier로 바꿀 수 있다는 이야기다.
- **Binary Softmax classifier**
  - 이전에 softmax 함수에 대해 배울때를 떠올려보자.

    ![ccxcv](\assets\images\cs231n\ccxcv.PNG)

  - 위와 같은 식을 **P(y<sub>i</sub> = 1 ㅣ x<sub>i</sub> ; w)**
  - 즉, 한 클래스에 대한 확률로 해석 해볼 수 있다.
  - 여기서는 **이항 분류(Binary)** 이기 때문에, 정답과 정답이 아닌 클래스 2개만 존재하여
  - 정답인 클래스의 확률이 **P(y<sub>i</sub> = 1 ㅣ x<sub>i</sub> ; w)**이라면
  - 정답이 아닌 클래스의 확률은 **P(y<sub>i</sub> = 0 ㅣ x<sub>i</sub> ; w) = 1 - P(y<sub>i</sub> = 1 ㅣx<sub>i</sub> ; w)**이다.
  - 둘의 확률을 더하면 1이 되는 것을 확인하자.
  - 여기서는 sigmoid 함수를 적용했기 때문에, 0~1 사이의 값이 결과로 나온다.
  - 따라서, 이것을 사용한 classifier의 예측은 중간 값인 0.5보다 큰지에 근거한다.
    
    - 간단하게, 0.5 보다 작으면 정답이 아니다, 0.5보다 크면 정답이다 라고 생각해 볼 수 있다.
- **Binary SVM classifier**
  
  - 뉴런의 출력에 max-margin hinge loss를 적용하여 Binary SVM이 되도록 훈련시킬 수 있다.
- **Regularization interpretation**
  - SVM/Softmax에서 규제 손실은 점진적 망각(gradual forgetting)에 비유할 수 있다고 한다.
  - 매 parameter 업데이트시 마다 시냅스 가중치 **w**가 0을 향해 줄어들기 때문이라고 한다.

- 한 줄로 정리하면 아래와 같다.

  > A single neuron can be used to implement a binary classifier



### 2-3. Commonly used activation functions
---
- 앞에서 언급했던대로 여러가지 **활성화 함수(activation functions)**에 대해 알아보자.
- 일단 활성화 함수는 숫자 하나를 입력으로 받아 고정된(fixed) 수학적 연산을 수행한다.



![6.3.sigmoid](\assets\images\cs231n\6.3.sigmoid.jpeg) ![6.3.tanh](\assets\images\cs231n\6.3.tanh.jpeg)	

- 첫번째 그래프는 시그모이드 함수의 그래프이다.
  - 함수값의 범위가 [0, 1] 사이에 위치한다.
- 두번째 그래프는 tanh 함수의 그래프이다.
  - 함수값의범위가 [-1, 1] 사이에 위치한다.
- 둘다 비선형성(non-linearity)을 띈다.
- 이제, 이 두가지 활성화 함수에 대해 자세히 알아보자



- **시그모이드(sigmoid) 함수**
  - 수식: **σ(x) = 1/(1 + e<sup>-x</sup>)**
  - 특징: 하나의 실수 값을 받아 0과 1사이의 값으로 뭉갠(squash)다.<br>매우 큰 양수는 1이되고, 매우 작은 음수는 0이 된다.<br>이 시그모이드 함수는 뉴런의 firing rate를 해석하기에 적합해서 예전에는 많이 쓰였다.<br>하지만 최근들어서는 크게 2가지의 문제점 때문에 예전처럼 많이 쓰이지는 않는다고 한다.



  - 문제점 1: **Sigmoids saturate and kill gradients**
    - 시그모이드 함수에 매우 큰 양수나, 매우 작은 음수가 들어오면 그 주변의 gradient는 0에 가까워진다.
    - 그래프의 개형을 보면 알 수 있다.
    - 역전파(backprop) 과정을 기억해보면, 이 값을 취하는 게이트에서 local gradient 값이 0에 가까워 진다는 말로 해석할 수 있다.
    - 그러면, 이 게이트 뒤에있는 게이트의 upstream gradient가 0에 가까워져서 경사 하강법(gradient descent)를 사용하여 가중치 업데이트가 일어나지 않게 된다.
    - 위와 같이 local gradient가 0에 가까워 지면서 발생하는 효과를 "kill gradients"라고 부른다.
    - "kill gradients"가 일어나면 더이상의 backward flow가 발생하지 않게된다.
    - 또한, 시그모이드 뉴런의 가중치를 초기화 할때 포화(saturate)가 발생하지 않도록 주의를 기울여야 하는데
    - 만약 너무 큰 값으로 가중치를 초기화 해버리면 뉴런이 포화상태가 되어 학습이 일어나지 않는다고 한다.
      - 가중치를 큰 값으로 설정하면, 그만큼 시그모이드 함수에 들어가는 값이 커지거나 작아져서 local gradient가 0에 가까워지는 kill gradients 효과가 발생할 수 있기 때문이다.



  - 문제점 2: **Sigmoid outputs are not zero-centered**
    - 위의 시그모이드 함수의 그래프를 다시보자 저 함수 값들을 평균내면 0.5이다.
    - 즉, non zero-centered라는 소리이다.
    - 또한, 항상 양수의 값을 가진다는 것을 알 수 있다. (매우 작은 음수의 경우는 0이 될 수도 있음)
    - 이렇게 **시그모이드 함수가 zero-centered 되지 않은 상황에서 모든 입력이 양수로 들어오면**,
    - **역전파(backprop)과정동안 gradient는 항상 양수이거나 음수가 되는 문제가 발생**한다.
    - 이는 **가중치 업데이트가 zig-zag 형태로 일어나게 만들어서 매우 비효율적**이다.
    - 이 문제는 첫번째 문제보다는 덜 치명적이라고 한다.

  - zig-zag 형태로 가중치 업데이트가 일어나는 예시를 가져와 보았다.

    ![dsfa](\assets\images\cs231n\dsfa.PNG)
    
    - 1, 3사분면이 가중치 업데이트가 가능한 방향이다.
    - 1사분면이 모든 gradient가 양수, 3사분면이 모든 gradient가 음수일 경우이다.
    - 이제 가중치 업데이트가 4사분면에 나타난 것 처럼 일어난다고 하면
    - 파란색 벡터처럼 가중치 업데이트가 일어나지 못하고,
    - 빨간색 벡터의 경로처럼 1,3사분면 방향으로만 움직여서 결국 zig-zag 형태로 나타난다는 것이다.



- **tanh 함수**
  - 2-3장의 초반에서 tanh의 그래프 모양을 다시 보자.
  - 시그모이드와 비슷하게 **tanh도 포화(saturate)와 관련된 문제가 발생**한다는 것을 알 수 있다.
  - 그러나 **시그모이드와는 다르게 tanh는 zero-centered 되어있다.**
  - 그래서, 실제로는 시그모이드 보다 tanh를 선호한다고 한다.
  - tanh은 시그모이드 함수로도 표현할 수 있다.
    - **tanh(x) = 2σ(2x) - 1**



- 또 다른 활성화 함수에 대해 알아보자.

  ![6.3.relu](\assets\images\cs231n\6.3.relu.jpeg)

  - 위는 ReLU 라고 부르는 활성화 함수의 개형이다.
  - 이제 ReLU, Leaky ReLu에 대해 알아보자.



- **ReLU 함수**
  - 수식: **f(x) = max(0, x)**
  - 먼저 그래프 개형을 보면, 앞의 시그모이드, tanh와 달리 포화되지 않는, 연산의 선형 영역을 가진다.(양수 부분)
  - 장점 1: 시그모이드/tanh 보다 SGD를 적용했을때 수렴속도가 빠르다.
    - 이는 앞에서 말한 선형성을 띄는것과, 포화(saturated)되지 않는 특성에서 비롯된다.
  - 장점 2: 시그모이드/tanh 보다 수식(계산)이 간단하다.
    - 단순히 0에서 활성화 행렬이 임계치를 가지도록 구현하면 되기 때문이다.
  - 단점 1: ReLU는 학습 과정중에 죽을(die)수도 있다.
    - die: 전체 training data set에 걸쳐 다시는 뉴런이 활성화가 되지 않는 상태
    - 이는 ReLU 활성화 함수의 그래프 개형과 관련된다.
    - 0 이하에서는 gradient가 0가 되어버리기 때문에, 이 unit(게이트)를 통과하는 gradient는 0이 되어버려서 가중치 업데이트가 일어나지 않기 때문이다.
    - 즉, ReLU unit은 data manifold에서 떨어져 학습중에 돌이킬수도 없이 죽어버릴 수 있따는 것이다.
  - 실제로, 학습 속도(learning rate)를 너무 높게 잡아버리면, 전체 네트워크의 40%가 죽어버리는 경우도 있다고 한다.
    - 따라서, ReLU unit을 사용할때는 학습속도를 잘 조절해줘야 한다.



- **Leaky ReLU 함수**
  - 수식: **f(x) = 1(x < 0)(ax) + 1(x >=0)(x)** , a는 작은 값의 상수
  - 앞에 ReLU 활성화 함수가 죽는 문제를 해결한 버전이다.
  - 이 Leaky ReLU가 더 좋은 성능을 낼때도 있지만, 항상 그렇지는 않다고 한다.

    

- **Maxout**
  - **ReLU와 그것의 leaky 버전을 일반화한것**
  - 수식: **max(w<sup>t</sup><sub>1</sub>x + b<sub>1</sub>, w<sup>t</sup><sub>2</sub>x + b<sub>2</sub>)** 
  - ReLU는 위에서 **w<sub>1</sub> ,b<sub>1</sub> = 0**인 특별한 경우이다.
  - Maxout 함수는 ReLU의 이점(포화되지 않는, 연산의 선형 영역)을 갖고,<br>단점(유닛이 죽는것)은 가지지 않는다.
  - 그러나 maxout도 단점이 있는데,<br>각 뉴런의 parameter 수가 2배로 늘어나서 그만큼 계산량이 많아진다는 것이다.



- 강의 노트에서는 아래와 같이 언급하였다.

  > TLDR: "What neuron type should I use?"<br>**Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of "dead" units in a network.**

- 모델 만들때 ReLU를 쓰자!




## 3. Neural Network architectures
- 본격적으로 신경망 구조에 대해 알아보자



### 3-1. Layer-wise organization
---
- **레이어 단위로 구성된 신경망**에 대해 살펴보겠다.
- **Neural Networks as neurons in graphs**
  - 신경망은 **비순환 그래프에 연결된 뉴런들의 집합체**로 모델링 될 수 있다.
   - 다른말로, 한 뉴런의 출력이 다른 뉴런의 입력이 될 수 있다는 것이다.
  - 이때, 비순환 그래프인 이유는 만약에 그래프내에 사이클(cycle)이 생긴다면 fordward pass에서 무한 루프를 야기하기 때문이다.
  - 신경망은 정형화된 층(layer)들로 구성된다.
   - 각 층 내에 뉴런들이 위치한다.
  - 일반적인 신경망에서 층의 종류는 **fully-connected layer**이다.
   - 두 개의 인접한 층 사이의 뉴런들이 전부 쌍으로 연결을 갖고, 같은 층 내에 있는 뉴런들은 연결을 갖지 않는 형태
- fully-connected layer를 그림으로 살펴보자
  
    ![6.4.1.neural_net](\assets\images\cs231n\6.4.1.neural_net.jpeg)
  
    - 인접한 두 개의 층 사이에는 뉴런들이 전부 쌍으로 연결을 갖고, 같은 층에 속한 뉴런 사이에는 연결을 갖지 않는 것을 확인할 수 있다.
    - 위의 그림 처럼 생긴 신경망을 **2-layer Neural Network**라고 부른다.



- **Naming conventions**
  - 보통 **N층의 신경망에 이름을 붙일 때, input 층을 세지 않는다.**
  - 따라서, 위의 그림에서는 input층을 빼고 은닉층과 출력층만 세서 2-layer 신경망이라 하는 것이다.
  - 신경망을 실제 뇌랑 비교하는데는 한계가 있기 때문에, 뉴런 대신에 unit이라는 표현을 쓴다고 한다.



- **Output layer**
  - **신경망의 다른 층들과 달리 출력층은 활성화 함수를 가지지 않는다.**
  - 보통, 출력층의 경우는 클래스 점수를 나타내거나 혹은 임의의 실수 값을 표현하기 위해 사용되기 때문이다.



- **Sizing neural networks**
  - 보통 **신경망의 크기를 계산할때는, 뉴런(units)의 개수를 세거나 parameter의 개수를 센다**고 한다.
    - 위의 신경망 사진을 예시로 들면,
    - 뉴런(units)의 개수를 셀 때는 입력층은 빼고 4(은닉층) + 2(출력층) = 6 이라고 계산하고
    - parameter의 개수를 셀 때는 [3 x 4] + [4 x 2] + bias(6개) = 26 이라고 계산한다.
  - 최근에 사용되는 Convolutional Networks의 경우는 약 1억개의 parameter와 10~20개의 층들로 구성된다고 한다.



### 3-2. Example feed-forward computation
---

- **신경망은 반복된 행렬 곱 연산과 활성화 함수가 섞인 형태로 구성**되어있다.

- 가장 큰 이유중 하나는, 위와 같은 형태로 구성하면 행렬 벡터 연산을 이용하여 신경망 계산을 할 때 매우 효율적이기 때문이다.

- 예시 그림을 통하여 알아보자.

  ![6.4.1.neural_net2](\assets\images\cs231n\6.4.1.neural_net2.jpeg)

  - 3-layer 신경망이다.
  - 입력은 [3 x 1] 벡터로 주어진다.
  - 먼저 은닉층 1의 가중치 행렬 W<sub>1</sub>이고 크기는 [4 x 3] 이다.
    - 이는 [3 x 1] 크기의 벡터 입력을 받아서 4개의 뉴런으로 보내야 하기 때문이다.
    - 모든 뉴런은 W<sub>1</sub>의 각 행에 각각의 가중치를 저장한다.
    - 따라서 np.dot(W<sub>1</sub>, x)는 그 층에 속한 모든 뉴런들의 활성화들을 계산한다.
  - 또한, 은닉층 1의 bias는 하나의 벡터 b<sub>1</sub>이고 크기는 [4 x 1] 이다.
    - 은닉층 1에 속한 4개의 뉴런에 bias가 필요하기 때문
  - 그 다음 은닉층 2의 가중치 행렬은 W<sub>2</sub>이고 크기는 [4 x 4] 이다.
    - [4 x 1] 크기의 벡터 입력을 받아 4개의 뉴런으로 보내야 하기 때문
    - 모든 뉴런은 W<sub>2</sub>의 각 행에 각각의 가중치를 저장한다.
  - 은닉층 2의 bias도 하나의 벡터 b<sub>2</sub>로 나타낼 수 있고 크기는 [4 x 1] 이다.
  - 마지막 출력층의 가중치 행렬 W<sub>3</sub>는 [1 x 4] 가 된다.
    - [4 x 1] 크기의 벡터 입력을 받아 하나의 뉴런으로 보내야 하기 때문
  - 출력층의 bias는 뉴런이 한개이기 때문에 [1 x 1] 크기의 벡터가 된다.
  - 이처럼 복잡한 full forward pass 과정을 3개의 행렬 곱 연산과 여기에 활성화 함수를 섞는 것으로 간단하게 표현할 수 있다.

- 행렬 곱 연산과 활성화 함수가 어떻게 섞이는지는 아래 코드에 나와있다.

```python
# forward-pass of a 3-layer neural network:
f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)
x = np.random.randn(3, 1) # random input vector of three numbers (3x1)
h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1)
h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1)
out = np.dot(W3, h2) + b3 # output neuron (1x1)
```

- 위에서 W<sub>1</sub>, W<sub>2</sub>, W<sub>3</sub>, b<sub>1</sub>, b<sub>2</sub>, b<sub>3</sub> 은 학습 가능한 parameter 이다.
- 변수 x는 training data의 전체 batch를 의미한다.
  
  - 각각의 입력은 x의 열이 된다.
- 앞에서 언급했던 것 처럼 출력층(out)에는 활성화 함수가 적용되지 않는다.

  > The forward pass of a fully-connected layer corresponds to one matrix multiplication followed by a bias offset and an activation function.





### 3-3. Representational power

---

- 네트워크의 표현력에 대해 알아보자.
- 