layout: post
title:  "[CS231n] 3. Optimization"
author: jspark
date: '2021-01-30 10:15:00 +0900'
category: CS231n
use_math: true

## 1. Optimization(최적화)

-   CS231n의 [유튜브 강의 내용](https://youtu.be/h7iBpEHGVNc)과 [강의 노트](https://cs231n.github.io/optimization-1/)를 참고하여 작성하였습니다.
-   이 포스트는 CS231n - Lecture3 의 내용을 포함합니다.
-   데스크탑이나 노트북의 경우 글꼴 크기를 확대(ctrl + 마우스 스크롤) 하여 보는 것을 추천합니다.
-   파이썬 코드는 글의 내용에 포함되지 않습니다.



### 1-1. Introduction

---

- 저번시간에 배웠던 두 가지 키 포인트를 기억하자
  1. **Score function**: 이미지의 픽셀 값들을 클래스 점수로 매핑해 주는 함수
  2. **Loss function**: 모델의 parameter들이 얼마나 좋은 성능을 내는가(얼마나 정답 label과 유사한 예측을 내 놓는가)에 대해 알려주는 함수


- **full SVM loss**

$$
L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + 1) \right] + \alpha R(W)
$$

- 이제 위의 내용을 바탕으로 마지막 요소인 **Optimization(최적화)**에 대해서 알아보자.
  - 최적화는 손실 함수의 값이 최소가 되도록 하는 가중치 **W**를 조절하는 과정이다.



### 1-2. Visualizing the loss function

---

- 손실 함수를 어떻게 시각화 할 것인지에 대한 내용이다.
- **CIFAR-10 (이미지 데이터 셋)**을 기억해보자, 우리는 이 데이터 셋을 training 시킬 때 가중치 **W**의 크기를 [10 x 3073]으로 두고 하였다. 그런데 이는 시각화 하기 어렵다. 최대 3차원 까지만 시각화가 가능하기 때문이다.
- 따라서, 이러한 고차원 상의 점들을 시각화 하기 위해 1차원(직선)이나 2차원(평면)으로 잘라서 보는 아이디어가 등장했다. 하나씩 살펴보자
- **1차원(직선)**
  - 먼저 가중치 행렬 **W**를 무작위로 하나 뽑고 이를 **W<sub>1</sub>**이라 하겠다.
  - 그렇다면 손실 함수는  **L(W + aW<sub>1</sub>)**와 같은 형태로 나타날 것이다.
    - 기존의 가중치 행렬 **W**가 있고, **W<sub>1</sub>**의 방향으로 **a**만큼 이동시키는 것.
    - **a**값이 x축, 손실 함수 값이 y축에 나타난다.
- **2차원(평면)**
  - 여기서도 가중치 행렬 **W**를 무작위로 하나더 뽑고 이를 **W<sub>2</sub>**라 하겠다.
  - 그렇다면 손실 함수는  **L(W + aW<sub>1</sub> + bW<sub>2</sub>)**와 같은 형태로 나타날 것이다.
    - **a,b** 값에 따라 **W<sub>1</sub>, W<sub>2</sub>**방향으로 움직이는 것 (a,b는 각각 x, y 축에 해당한다.)
- 그림으로 한번에 나타내면 아래와 같다.

![svmloss1_2d](C:\Users\idpjs\Desktop\svmloss1_2d.PNG)

- 첫 번째 사진(왼쪽)은 1차원 상에서 a 값이 변화함에 따라 loss function 값의 변화를 보여주는 것이고
- 나머지 두개는 2차원 평면 상에서 a,b 값의 변화에 따른 loss function 값을 보여준다.
  - 파란색이 낮은 손실, 빨간색이 높은 손실을 의미한다.



- 첫 번째 사진을 자세히 보자, 그러면 **부분적으로 선형(piecewise linear)**임을 알 수 있다.
  - **Linearlity는 f(aX + bY) = af(X) + bf(Y)**로 나타난 다는 것을 기억하자(선형대수)
- 이렇게 부분적으로 선형이 나타나는 이유를 손실 함수의 수식으로 설명할 수 있다.

$$
L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + 1) \right]
$$

- 위의 식에서도 알 수 있듯이 data loss L<sub>i</sub>는 **W**의 선형함수들의 합으로 표현된다.
  - **W**의 선형함수는 **f(x<sub>i</sub>; W)<sub>j</sub>**꼴로 나타났던 것을 기억하자.
- 더 명확히 표현하기 위해 데이터 셋이 3개의 1차원 상의 점들과 3개의 클래스로 구성됬다고 하자,<br>규제가 없다고 했을 때 총 SVM loss는 아래와 같이 나타난다.

$$
% <![CDATA[
\begin{align}
L_0 = & \max(0, w_1^Tx_0 - w_0^Tx_0 + 1) + \max(0, w_2^Tx_0 - w_0^Tx_0 + 1) \\\\
L_1 = & \max(0, w_0^Tx_1 - w_1^Tx_1 + 1) + \max(0, w_2^Tx_1 - w_1^Tx_1 + 1) \\\\
L_2 = & \max(0, w_0^Tx_2 - w_2^Tx_2 + 1) + \max(0, w_1^Tx_2 - w_2^Tx_2 + 1) \\\\
L = & (L_0 + L_1 + L_2)/3
\end{align} %]]>
$$

- 여기서 **w<sub>j</sub>와 x**는 실제로 1차원[1 x 1]이기 때문에 **w**를 전치(transpose)해주지 않아도 똑같은 의미가 된다.
  - x는 1차원 상의 점이라고 가정하였고, 가중치 **W**는 3개의 클래스를 구분해야 하기 때문에 [3 x 1] 행렬이 되는데, 여기서 **w<sub>j</sub>**는 행 하나를 뽑은것이기 때문에 [1 x 1]이 된다.
- 위의 식에서 **w<sub>0</sub>**를 보면 max(0, -) 내에 속해있기 때문에 0을 기준으로 꺾인다는 것을 예상할 수 있다.
  - 실제 시각화한 그림은 아래와 같다.
   ![svmbowl](C:\Users\idpjs\Desktop\svmbowl.png)
  - x축은 가중치 혹은 parameter이고, y축은 loss 값이다.
  - 선들을 보면 **max(0, - )**의 특성상 0(loss값)을 기준으로 꺾인다는 것을 확인 할 수 있다.
  - 서로 다른 3가지 색의 loss(L<sub>1</sub>, L<sub>2</sub>, L<sub>3</sub>)를 합치면 오른쪽의 아래로 볼록한 그래프가 생기는 것을 확인 할 수 있다.
  - 이런 개형의 함수를 볼록 함수(convex function)이라고 한다.
  - 이 score function을 neural networks로 확장시키면 더이상 볼록 함수의 형태가 아니라 울퉁불퉁하고 복잡한 형태를 띈다고 한다.
- **미분 불가능한 손실 함수**의 경우도 살펴보자
  - 위의 그래프에서도 볼 수 있듯이 max(0, -)의 특성상 0이 되는 지점에서 미분이 불가능해진다.
  - 따라서 이 지점에서 gradient 값이 존재하지 않아 subgradient를 사용한다고 한다.
  - 강의 노트에서는 앞으로 gradient = subgradient라고 생각하면 된다고 한다.



## 2. Optimization

- 이제 이 글의 주제인 최적화에 대해 다뤄보자
- 시작하기 전에 최적화의 목적을 다시한번 상기하자.

  > The goal of optimization is to find **W** that minimizes the loss function.

- 강의 노트에서는 이 section 에서는 주로 볼록 함수(convex function) 형태를 다루기 때문에 나중에 신경망의 최적화를 할 때는 여기서 나온 방법을 적용하기 어렵다는 것을 알아두라고 한다.



### 2-1. [Strategy #1] A first very bad idea solution: Random search

---

- 어떻게 모델의 가중치 **W**를 최적화 할 수 있을까?
- 가장 간단하게는 무작위로 **W** 값을 넣어보며 성능이 좋은것을 찾으면 된다!
- 실제로 10개의 클래스를 구분해야 하는 분류기에 무작위 방법으로 정한 **W**를 적용시키고 test set에 넣어보면 **15.5%**의 정확도가 나온다고 한다. (그냥 찍는게 10%니 이것보다는 좋다...)
- 이처럼 무작위로 계속 **W**값을 넣어보면서 찾으면 실제 성능이 나쁘게 나올 뿐만 아니라 복잡한 신경망의 경우는 경우의수가 많아서 시간적으로도 비효율적이다.

- 따라서 **iterative refinement**방식이 고안되었다.
  - 처음에 **W**를 무작위로 뽑고서 더 좋은 성능을 내도록 조금씩 개선시키는 것이다.

    > **Our strategy will be start with random weights and iteratively refine them over time to get lower loss**

  - 강의 노트에서는 **눈을 가리고 하산** 하는것에 비유했다.
  - 여기서 loss 값은 고도가 된다.
  - 매 위치에서 한발짝 움직였을때 고도가 가장 낮아지는 지점을 찾아 계속 이동하다보면 결국 평지에 도착할 것이라는 얘기다.



### 2-2. [Strategy #2] Random Local Search

---

